{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un RAG en Local sur un ensemble de papier de recherche en Reinforcement Deep Learning \n",
    "\n",
    "OpenAI liste les principaux papiers du domaine sur cette page : https://spinningup.openai.com/en/latest/spinningup/keypapers.html\n",
    "On rajoutera aussi le livre de Sutton et al.: http://incompleteideas.net/book/RLbook2020.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Scrapping des pdf\n",
    "### Récupération des liens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liens trouvés:  ['https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf', 'https://arxiv.org/abs/1507.06527', 'https://arxiv.org/abs/1511.06581', 'https://arxiv.org/abs/1509.06461', 'https://arxiv.org/abs/1511.05952', 'https://arxiv.org/abs/1710.02298', 'https://arxiv.org/abs/1602.01783', 'https://arxiv.org/abs/1502.05477', 'https://arxiv.org/abs/1506.02438', 'https://arxiv.org/abs/1707.06347', 'https://arxiv.org/abs/1707.02286', 'https://arxiv.org/abs/1708.05144', 'https://arxiv.org/abs/1611.01224', 'https://arxiv.org/abs/1801.01290', 'http://proceedings.mlr.press/v32/silver14.pdf', 'https://arxiv.org/abs/1509.02971', 'https://arxiv.org/abs/1802.09477', 'https://arxiv.org/abs/1707.06887', 'https://arxiv.org/abs/1710.10044', 'https://arxiv.org/abs/1806.06923', 'https://openreview.net/forum?id=ByG_3s09KX', 'https://github.com/google/dopamine', 'https://arxiv.org/abs/1611.02247', 'https://arxiv.org/abs/1710.11198', 'https://arxiv.org/abs/1802.10031', 'https://arxiv.org/abs/1702.08892', 'https://arxiv.org/abs/1707.01891', 'https://arxiv.org/abs/1611.01626', 'https://arxiv.org/abs/1704.04651', 'http://papers.nips.cc/paper/6974-interpolated-policy-gradient-merging-on-policy-and-off-policy-gradient-estimation-for-deep-reinforcement-learning', 'https://arxiv.org/abs/1704.06440', 'https://arxiv.org/abs/1703.03864', 'https://arxiv.org/abs/1605.09674', 'https://arxiv.org/abs/1606.01868', 'https://arxiv.org/abs/1703.01310', 'https://arxiv.org/abs/1611.04717', 'https://arxiv.org/abs/1703.01260', 'https://arxiv.org/abs/1705.05363', 'https://arxiv.org/abs/1808.04355', 'https://arxiv.org/abs/1810.12894', 'https://arxiv.org/abs/1611.07507', 'https://arxiv.org/abs/1802.06070', 'https://arxiv.org/abs/1807.10299', 'https://arxiv.org/abs/1606.04671', 'http://proceedings.mlr.press/v37/schaul15.pdf', 'https://arxiv.org/abs/1611.05397', 'https://arxiv.org/abs/1707.03300', 'https://arxiv.org/abs/1701.08734', 'https://arxiv.org/abs/1707.07907', 'https://openreview.net/forum?id=rk07ZXZRb&noteId=rk07ZXZRb', 'https://arxiv.org/abs/1707.01495', 'https://arxiv.org/abs/1606.04695', 'https://arxiv.org/abs/1703.01161', 'https://arxiv.org/abs/1805.08296', 'https://arxiv.org/abs/1606.04460', 'https://arxiv.org/abs/1703.01988', 'https://arxiv.org/abs/1702.08360', 'https://arxiv.org/abs/1803.10760', 'https://arxiv.org/abs/1806.01822', 'https://arxiv.org/abs/1707.06203', 'https://arxiv.org/abs/1708.02596', 'https://arxiv.org/abs/1803.00101', 'https://arxiv.org/abs/1807.01675', 'https://openreview.net/forum?id=SJJinbWRZ&noteId=SJJinbWRZ', 'https://arxiv.org/abs/1809.05214', 'https://arxiv.org/abs/1809.01999', 'https://arxiv.org/abs/1712.01815', 'https://arxiv.org/abs/1705.08439', 'https://arxiv.org/abs/1611.02779', 'https://arxiv.org/abs/1611.05763', 'https://arxiv.org/abs/1703.03400', 'https://openreview.net/forum?id=B1DmUzWAW&noteId=B1DmUzWAW', 'https://arxiv.org/abs/1803.02811', 'https://arxiv.org/abs/1802.01561', 'https://openreview.net/forum?id=H1Dy---0Z', 'https://openreview.net/forum?id=r1lyTjAqYX', 'https://arxiv.org/abs/1712.09381', 'https://ray.readthedocs.io/en/latest/rllib.html', 'https://arxiv.org/abs/1809.07731', 'https://arxiv.org/abs/1808.00177', 'https://arxiv.org/abs/1806.10293', 'https://arxiv.org/abs/1811.00260', 'https://arxiv.org/abs/1606.06565', 'https://arxiv.org/abs/1706.03741', 'https://arxiv.org/abs/1705.10528', 'https://arxiv.org/abs/1801.08757', 'https://arxiv.org/abs/1707.05173', 'https://arxiv.org/abs/1711.06782', 'http://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf', 'https://arxiv.org/abs/1603.00448', 'https://arxiv.org/abs/1606.03476', 'https://xbpeng.github.io/projects/DeepMimic/2018_TOG_DeepMimic.pdf', 'https://arxiv.org/abs/1810.00821', 'https://arxiv.org/abs/1810.05017', 'https://arxiv.org/abs/1604.06778', 'https://arxiv.org/abs/1708.04133', 'https://arxiv.org/abs/1709.06560', 'https://arxiv.org/abs/1810.02525', 'https://arxiv.org/abs/1811.02553', 'https://arxiv.org/abs/1803.07055', 'https://arxiv.org/abs/1907.02057', 'https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf', 'http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf', 'http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Neural-Netw-2008-21-682_4867%5b0%5d.pdf', 'https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf', 'https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf', 'https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Récupération des liens des pdfs du site\n",
    "url = \"https://spinningup.openai.com/en/latest/spinningup/keypapers.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "pdf_links = []\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    links = [a['href'] for a in soup.find_all('a', class_='reference external') if 'href' in a.attrs]\n",
    "    print(\"liens trouvés: \", links)\n",
    "else:\n",
    "    print(\"Erreur lors du téléchargement de la page\")\n",
    "\n",
    "# scrapping dans arkiv et openreview\n",
    "for link in links:\n",
    "\n",
    "    if link.startswith(\"https://arxiv.org/\") or link.startswith(\"https://openreview\"):\n",
    "        a_class,pref = (\"abs-button download-pdf\",\"https://arxiv.org\") if link.startswith(\"https://arxiv.org/\") else (\"note_content_pdf\",\"https://openreview.net\")\n",
    "        r = requests.get(link)\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.text,'html.parser')\n",
    "            pdf_link = [a['href'] for a in soup.find_all('a', class_=a_class) if 'href' in a.attrs]\n",
    "            pdf_links.append(pref+pdf_link[0])\n",
    "        else:\n",
    "            print(\"problem\")\n",
    "\n",
    "    elif  link.endswith(\".pdf\"):\n",
    "        pdf_links.append(link)\n",
    "\n",
    "pdf_links.append(\"http://incompleteideas.net/book/RLbook2020.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement des pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:00, 182.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [00:05, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n",
      "probleme de téléchargement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(\"data/\"):\n",
    "    os.mkdir(\"data/\")\n",
    "\n",
    "for (i,pdf_link) in tqdm(enumerate(pdf_links)):\n",
    "    path = \"data/papier_\"+str(i)+ \".pdf\"\n",
    "    \n",
    "    if not os.path.isfile(path):\n",
    "        pdf_r = requests.get(pdf_link)\n",
    "        if pdf_r.status_code == 200:\n",
    "            with open(path,\"wb\") as f:\n",
    "                f.write(pdf_r.content)\n",
    "    else:\n",
    "        print(\"probleme de téléchargement\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "def text_to_sentences(text):\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sentences = [str(s) for s in list(doc.sents)]\n",
    "    return sentences\n",
    "\n",
    "def sentence_list_to_chunk(nb_sentence :  int,s_list : list[str]):\n",
    "    chunks = [\" \".join(s_list[i:i+nb_sentence]) for i in range(0,len(s_list),nb_sentence)]\n",
    "    return chunks\n",
    "\n",
    "text = \"Même si des études récentes montrent qu’il est possible d’utiliser des résolutions plus fines pour des simulations climatiques, la majorité des modèles de climat utilisent une résolution horizontale de l’ordre de 150 km à 50 km. À ces résolutions, seuls les processus de grande échelle sont décrits par les équations primitives discrétisées. Pour représenter les processus de fine échelle, non explicitement résolus par le modèle, comme la convection profonde, la turbulence ou la microphysique, la démarche classique consiste à utiliser des paramétrisations physiques. Les paramétrisations physiques sont des ensembles d’équations, empiriques ou théoriques, développées à partir d’une combinaison de données issues d’observations, de modèles conceptuels, de résultats de modèles à haute résolution, et/ou d’approches théoriques.Une nouvelle méthode permettant de développer de nouvelles paramétrisations consiste à utiliser des techniques de machine learning, en particulier des réseaux de neurones (NN). Les NN pourraient réduire certains biais connus des paramétrisations physiques. Mais la cohérence physique des paramétrisations NN peut être insuffisante. Cela se traduit alors par l’apparition de biais (et/ou d’explosion numérique) dans les simulations climatiques.Il existe une famille de techniques permettant d’implémenter des contraintes physiques aux NN pendant l’apprentissage, appelée « NNs informés par la physique » (ou « physics informed neural networks », PINNs). Dans ce stage, nous proposons d’explorer ces approches dans le cadre du développement d’une paramétrisation NN pour la convection profonde. Pour ce faire, nous utiliserons la paramétrisation de convection profonde de notre modèle comme cadre idéal de travail. Il s’agira de mettre en place et tester la valeur ajoutée de contraintes physiques, à travers l’utilisation d’une des deux méthodes suivantes : (i) via un terme de pénalité ajouté à la loss pendant l’apprentissage ou (ii) en ajoutant une couche spécifique à la fin du NN dont les paramètres sont imposés manuellement.\"\n",
    "text = text + text\n",
    "s_list = text_to_sentences(text)\n",
    "\n",
    "print(len(s_list))\n",
    "chunks = sentence_list_to_chunk(10,s_list)\n",
    "print(len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"data/\"\n",
    "pdf_files = [\"data/\"+f for f in os.listdir(data_path) if f.endswith('.pdf')]\n",
    "pdf_dic = {}\n",
    "\n",
    "def text_formatter(text:str) -> str:\n",
    "    cleaned_text = text.replace(\"\\n\",\" \").strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    pages_and_text = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    for (page_number,page) in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text=text)\n",
    "        sentences = text_to_sentences(text)\n",
    "        chunks = sentence_list_to_chunk(10,sentences)\n",
    "        pages_and_text.append({\n",
    "            \"page_number\": page_number,\n",
    "            \"text\": text,\n",
    "            \"chunks\": chunks,\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_words_count\": len(text.split(\" \")),\n",
    "            \"page_sentences_count\": len(text.split(\". \")),\n",
    "            \"page_token_count\": len(text)/4,\n",
    "            \"page_chunks_count\": len(chunks)\n",
    "        })\n",
    "    return pages_and_text\n",
    "\n",
    "def open_pdfs(pdf_path_list : list[str]) -> list[dict]:\n",
    "    pdf_and_text = []\n",
    "    pdfs_pages_text_list = []\n",
    "    for (pdf_number,pdf_path) in enumerate(pdf_path_list):\n",
    "\n",
    "        pages_and_text = open_and_read_pdf(pdf_path)\n",
    "        pdfs_pages_text_list.append(pages_and_text)\n",
    "        df = pd.DataFrame(pages_and_text)\n",
    "        dict_mean = df.describe().round(2).loc['mean'].to_dict()\n",
    "\n",
    "        doc = fitz.open(pdf_path)\n",
    "        texte_complet = \"\".join([page.get_text(\"text\") for page in doc])\n",
    "        texte_complet = text_formatter(texte_complet)\n",
    "        pdf_and_text.append({\n",
    "            \"pdf_number\":pdf_number,\n",
    "            \"pdf_page_count\":doc.page_count,\n",
    "            \"pdf_char_count\":len(texte_complet),\n",
    "            \"pdf_word_count\":len(texte_complet.split(\" \")),\n",
    "            \"pdf_sentence_count_raw\":len(texte_complet.split(\". \")),\n",
    "            \"pdf_token_count\": len(texte_complet) /4,\n",
    "            \"page_mean_char_count\": dict_mean[\"page_char_count\"],\n",
    "            \"page_mean_words_count\": dict_mean[\"page_words_count\"],\n",
    "            \"page_mean_sentences_count\": dict_mean[\"page_sentences_count\"],\n",
    "            \"page_mean_token_count\": dict_mean[\"page_token_count\"]\n",
    "        })\n",
    "    return pdf_and_text,pdfs_pages_text_list\n",
    "\n",
    "pdf_and_text,pdfs_pages_text_list = open_pdfs(pdf_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_sentences_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_chunks_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4000.78</td>\n",
       "      <td>647.33</td>\n",
       "      <td>32.89</td>\n",
       "      <td>1000.19</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.74</td>\n",
       "      <td>561.28</td>\n",
       "      <td>120.60</td>\n",
       "      <td>17.09</td>\n",
       "      <td>140.32</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>3183.00</td>\n",
       "      <td>459.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>795.75</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3665.00</td>\n",
       "      <td>546.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>916.25</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.00</td>\n",
       "      <td>3954.00</td>\n",
       "      <td>677.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>988.50</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.00</td>\n",
       "      <td>4178.00</td>\n",
       "      <td>746.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>1044.50</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.00</td>\n",
       "      <td>4906.00</td>\n",
       "      <td>795.00</td>\n",
       "      <td>76.00</td>\n",
       "      <td>1226.50</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_words_count  page_sentences_count  \\\n",
       "count         9.00             9.00              9.00                  9.00   \n",
       "mean          4.00          4000.78            647.33                 32.89   \n",
       "std           2.74           561.28            120.60                 17.09   \n",
       "min           0.00          3183.00            459.00                 19.00   \n",
       "25%           2.00          3665.00            546.00                 22.00   \n",
       "50%           4.00          3954.00            677.00                 29.00   \n",
       "75%           6.00          4178.00            746.00                 34.00   \n",
       "max           8.00          4906.00            795.00                 76.00   \n",
       "\n",
       "       page_token_count  page_chunks_count  \n",
       "count              9.00               9.00  \n",
       "mean            1000.19               3.67  \n",
       "std              140.32               1.80  \n",
       "min              795.75               2.00  \n",
       "25%              916.25               3.00  \n",
       "50%              988.50               3.00  \n",
       "75%             1044.50               4.00  \n",
       "max             1226.50               8.00  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pdfs_pages_text_list[0])\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des chunks de phrases. \n",
    "On parcours les toutes les pages des pdfs. Pour chaque page, on prends tout le texte on le sépare en phrases qu'on met dans une liste, on crée ensuite une liste donc chaque élément est une liste de 10 phrases. On met ca dans un dictionnaire avec les données suivante: le nom du papier, le numéro de page absolue du papier, le lien du papier (local). On fait une liste du dictionnaire pour toutes les pages d'un papier. On rajoute dans cette liste tout les autres pdfs. Ca nous donne un gros dataframe avec un ensemble de listes de phrases. Apprès on prend ces chunks on les join et on crée u..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_number</th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7825.00</td>\n",
       "      <td>7825.00</td>\n",
       "      <td>7825.00</td>\n",
       "      <td>7825.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>45.08</td>\n",
       "      <td>75.35</td>\n",
       "      <td>979.19</td>\n",
       "      <td>244.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33.16</td>\n",
       "      <td>141.44</td>\n",
       "      <td>590.59</td>\n",
       "      <td>147.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>515.00</td>\n",
       "      <td>128.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>43.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>957.00</td>\n",
       "      <td>239.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>77.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>1366.00</td>\n",
       "      <td>341.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>102.00</td>\n",
       "      <td>547.00</td>\n",
       "      <td>7724.00</td>\n",
       "      <td>1931.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pdf_number  page_number  chunk_count  chunk_token_count\n",
       "count     7825.00      7825.00      7825.00            7825.00\n",
       "mean        45.08        75.35       979.19             244.80\n",
       "std         33.16       141.44       590.59             147.65\n",
       "min          0.00         0.00         1.00               0.25\n",
       "25%          7.00         5.00       515.00             128.75\n",
       "50%         43.00        10.00       957.00             239.25\n",
       "75%         77.00        48.00      1366.00             341.50\n",
       "max        102.00       547.00      7724.00            1931.00"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pdfs_to_chunks(data_path,pdfs_pages_text_list):\n",
    "    # pdf_files = [\"data/\"+f for f in os.listdir(data_path) if f.endswith('.pdf')]\n",
    "    chunks = []\n",
    "    # pdf_and_text,pdfs_pages_text_list = open_pdfs(pdf_files)\n",
    "    \n",
    "    for (pdf_num,pdf) in enumerate(pdfs_pages_text_list):\n",
    "        for page in pdf:\n",
    "            for chunk in page[\"chunks\"]:\n",
    "                chk = {}\n",
    "                chk[\"pdf_number\"] = pdf_num\n",
    "                chk[\"page_number\"] = page[\"page_number\"]\n",
    "                chk[\"text\"] = chunk\n",
    "                chk[\"chunk_count\"] = len(chk[\"text\"])\n",
    "                chk[\"chunk_token_count\"] = len(chk[\"text\"])/4\n",
    "\n",
    "                chunks.append(chk)\n",
    "\n",
    "    return chunks\n",
    "            \n",
    "chunks_list = pdfs_to_chunks(\"dddaz\",pdfs_pages_text_list)\n",
    "df = pd.DataFrame(chunks_list)\n",
    "df.describe().round(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_number</th>\n",
       "      <th>page_number</th>\n",
       "      <th>text</th>\n",
       "      <th>chunk_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Playing Atari with Deep Reinforcement Learning...</td>\n",
       "      <td>1855</td>\n",
       "      <td>463.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>However reinforcement learning presents severa...</td>\n",
       "      <td>1327</td>\n",
       "      <td>331.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Figure 1: Screen shots from ﬁve Atari 2600 Gam...</td>\n",
       "      <td>1661</td>\n",
       "      <td>415.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>K}. The action is passed to the emulator and m...</td>\n",
       "      <td>1184</td>\n",
       "      <td>296.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>As a result, we can apply standard reinforceme...</td>\n",
       "      <td>1107</td>\n",
       "      <td>276.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pdf_number  page_number                                               text  \\\n",
       "0           0            0  Playing Atari with Deep Reinforcement Learning...   \n",
       "1           0            0  However reinforcement learning presents severa...   \n",
       "2           0            1  Figure 1: Screen shots from ﬁve Atari 2600 Gam...   \n",
       "3           0            1  K}. The action is passed to the emulator and m...   \n",
       "4           0            1  As a result, we can apply standard reinforceme...   \n",
       "\n",
       "   chunk_count  chunk_token_count  \n",
       "0         1855             463.75  \n",
       "1         1327             331.75  \n",
       "2         1661             415.25  \n",
       "3         1184             296.00  \n",
       "4         1107             276.75  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(chunks_list)\n",
    "df.describe().round(2)\n",
    "df = df[ df[\"chunk_token_count\"] >= 20]\n",
    "len(df)\n",
    "df = df[df[\"chunk_token_count\"] <= 600]\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
